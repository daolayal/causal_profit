{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diegoolaya/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/Users/diegoolaya/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.ensemble.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full._init_ import *\n",
    "os.getcwd()\n",
    "\"\"\"\n",
    "Generate a synthetic dataset.\n",
    "\"\"\"\n",
    "# get_syn = make_uplift_classification(n_samples = 5000,\n",
    "#                                      treatment_name=['control', 'treatment1'],\n",
    "#                                      n_uplift_increase_dict = {'treatment1': 10},\n",
    "#                                      n_classification_features = 5,\n",
    "#                                      n_classification_informative = 3,\n",
    "#                                      positive_class_proportion=0.3,\n",
    "#                                      #n_uplift_increase_mix_informative_dict={'treatment1': 2},\n",
    "#                                      delta_uplift_increase_dict={'treatment1': 0.07},\n",
    "#                                      y_name = ['Y'],\n",
    "#                                      random_seed = 100)\n",
    "\n",
    "get_syn = make_uplift_classification(n_samples = 5000,\n",
    "                                     treatment_name=['control', 'treatment1'],\n",
    "                                     n_uplift_increase_dict = {'treatment1': 10},\n",
    "                                     n_classification_features = 5,\n",
    "                                     n_classification_informative = 3,\n",
    "                                     positive_class_proportion=0.5,\n",
    "                                     #n_uplift_increase_mix_informative_dict={'treatment1': 2},\n",
    "                                     delta_uplift_increase_dict={'treatment1': 0.07},\n",
    "                                     y_name = ['Y'],\n",
    "                                     random_seed = 100)\n",
    "\n",
    "\"\"\"\n",
    "Elements.\n",
    "\"\"\"\n",
    "df = get_syn[0]\n",
    "T = \"treatment_group_key\"\n",
    "df[T] = np.where(df[T] == \"treatment1\",1,0)\n",
    "Y = \"Y\"\n",
    "X = get_syn[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x1_informative',\n",
       " 'x2_informative',\n",
       " 'x3_informative',\n",
       " 'x4_irrelevant',\n",
       " 'x5_irrelevant',\n",
       " 'x6_uplift_increase',\n",
       " 'x7_uplift_increase',\n",
       " 'x8_uplift_increase',\n",
       " 'x9_uplift_increase',\n",
       " 'x10_uplift_increase',\n",
       " 'x11_uplift_increase',\n",
       " 'x12_uplift_increase',\n",
       " 'x13_uplift_increase',\n",
       " 'x14_uplift_increase',\n",
       " 'x15_uplift_increase',\n",
       " 'x16_increase_mix']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "treatment_group_key     0     1\n",
      "Y                              \n",
      "0                    2519  2110\n",
      "1                    2481  2890\n",
      "Total number of observations = 10000\n",
      "Number of control group observations =  5000\n",
      "Number of treatment group observations =  5000\n",
      "Positive rate treatment =  0.578\n",
      "Positive rate control =  0.4962\n",
      "Overall effect =  0.08179999999999998\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Descriptive\n",
    "\"\"\"\n",
    "desc_table = pd.crosstab(index = df[Y], columns = df[T], margins = False)\n",
    "print(desc_table)\n",
    "print(\"Total number of observations =\",df.shape[0])\n",
    "#print(\"Total number of variables = \",len(X))\n",
    "print(\"Number of control group observations = \",df[T].value_counts()[0])\n",
    "print(\"Number of treatment group observations = \",df[T].value_counts()[1])\n",
    "print(\"Positive rate treatment = \",desc_table.loc[1][1]/(desc_table.loc[1][1] + desc_table.loc[0][1]))\n",
    "print(\"Positive rate control = \",desc_table.loc[1][0]/(desc_table.loc[1][0] + desc_table.loc[0][0]))\n",
    "print(\"Overall effect = \",desc_table.loc[1][1]/(desc_table.loc[1][1] + desc_table.loc[0][1])-desc_table.loc[1][0]/(desc_table.loc[1][0] + desc_table.loc[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07999999999999996"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.57-0.49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5 folds stratified CV\n",
    "\"\"\"\n",
    "Amount_folds = 5\n",
    "#dump(folds,'folds.sav')\n",
    "# CV\n",
    "folds = create_folds(df,Amount_folds,T,Y,X)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Causal models\n",
    "\"\"\"\n",
    "random_state = 100\n",
    "model_names = ['CF','T_learner','S_learner']\n",
    "#xgb.XGBClassifier(objective = \"binary:logistic\",n_estimators = 100, random_state = random_state)\n",
    "#LogisticRegression(solver='saga',max_iter=3000, random_state = random_state)\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "algorithm_1 = LogisticRegression(solver='saga',max_iter=3000, random_state = random_state)\n",
    "algorithm_2 = LogisticRegression(solver='saga',max_iter=3000, random_state = random_state)\n",
    "algorithm_3 = LogisticRegression(solver='saga',max_iter=3000, random_state = random_state)\n",
    "algorithm_4 = LogisticRegression(solver='saga',max_iter=3000, random_state = random_state)\n",
    "####################################################\n",
    "\n",
    "effect_estimates = []\n",
    "for model in model_names:\n",
    "    effect_estimates.append(fit_causal(folds,T,Y,X,model,algorithm_1,algorithm_2,algorithm_3,algorithm_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(effect_estimates,'effect_estimates_LR.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Performance\n",
    "\"\"\"\n",
    "import statistics \n",
    "\"\"\"\n",
    "There is two algoritms:\n",
    "> LR: call \"effect_estimates_LR.sav\"\n",
    "> XGB: call \"effect_estimates_XGB.sav\"\n",
    "\"\"\"\n",
    "effect_estimates= load(\"effect_estimates_XGB.sav\")\n",
    "\"\"\"\n",
    "and three causal models (CF,T_learner,S_learner) where:\n",
    "> CF = effect_estimates[0]\n",
    "> T_learner = effect_estimates[1]\n",
    "> S_learner = effect_estimates[2]\n",
    "\n",
    "\"\"\"\n",
    "model_predictions = effect_estimates[0]\n",
    "\n",
    "\"\"\"\n",
    "The OB and TC matrices\n",
    "\"\"\"\n",
    "def create_OB(b_11,b_10,b_01,b_00):\n",
    "    OB = [[b_00,b_01],[b_10,b_11]]\n",
    "    return OB;\n",
    "\n",
    "def create_TC(c_11,c_10,c_01,c_00):\n",
    "    TC = [[c_00,c_01],[c_10,c_11]]\n",
    "    return TC;\n",
    "\n",
    "\"\"\"\n",
    "Specify the b and c parameters:\n",
    "\"\"\"\n",
    "#b = [10,100,200]\n",
    "b = [100]\n",
    "#benefit = 10\n",
    "#c_pro = [0.2,0.3,0.4]\n",
    "c = 0.1\n",
    "\n",
    "av_qini_insensitive = []\n",
    "av_qini_sensitive = []\n",
    "std_qini_insensitive = []\n",
    "std_qini_sensitive = []\n",
    "av_profit_insensitive = []\n",
    "av_profit_sensitive = []\n",
    "std_profit_insensitive = []\n",
    "std_profit_sensitive = []\n",
    "max_profit_sensitive = []\n",
    "max_profit_insensitive = []\n",
    "MP_sen_index = []\n",
    "MP_sen_val = []\n",
    "MP_in_index = []\n",
    "MP_in_val = []\n",
    "\n",
    "for benefit in b:\n",
    "    \n",
    "    \"\"\"\n",
    "    Equal benefits\n",
    "    \"\"\"\n",
    "    benefit_pos = benefit\n",
    "    benefit_neg = benefit*1.2\n",
    "    cost_pos = benefit_pos*c\n",
    "    cost_neg = benefit_pos*c\n",
    "    \n",
    "    \"\"\"\n",
    "    OB and TC matrices\n",
    "    \"\"\"\n",
    "    OB = create_OB(benefit_pos,benefit_neg,0,0)\n",
    "    TC = create_TC(cost_pos,0,cost_neg,0)\n",
    "    #TC = create_TC(benefit_pos*c,0,benefit_pos*c,0)\n",
    "    \n",
    "    \"\"\"\n",
    "    Gamma and delta:\n",
    "    \"\"\"\n",
    "    gamma = (OB[0][0] - TC[0][0] - OB[0][1] + TC[0][1])/(OB[1][0] - TC[1][0] - OB[0][0] + TC[0][0])\n",
    "    delta = (OB[1][0] - TC[1][0] + OB[0][1] - TC[0][1] - OB[1][1] + TC[1][1] - OB[0][0] + TC[0][0])/(OB[1][0] - TC[1][0] - OB[0][0] + TC[0][0])\n",
    "\n",
    "    qini_sensitive = []\n",
    "    qini_insensitive = []\n",
    "    profit_sensitive = []\n",
    "    profit_insensitive = []\n",
    "    list_sen_prof = []\n",
    "    list_ins_prof = []\n",
    "    for fold in list(range(len(model_predictions))):\n",
    "        \"\"\"\n",
    "        Qini\n",
    "        \"\"\"\n",
    "        perf_table_sensitive = performance_profit(model_predictions,gamma,delta,True,\"qini\",\"Profit-sensitive\")\n",
    "        perf_table_insensitive = performance_profit(model_predictions,gamma,delta,False,\"qini\",\"Profit-insensitive\")\n",
    "        qini_sensitive.append(qini_metric(perf_table_sensitive[fold]))\n",
    "        qini_insensitive.append(qini_metric(perf_table_insensitive[fold]))\n",
    "        \"\"\"\n",
    "        Profit\n",
    "        \"\"\"\n",
    "        prof_table_sensitive = performance_profit(model_predictions,gamma,delta,True,\"Profit\",\"Profit-sensitive\")\n",
    "        prof_table_insensitive = performance_profit(model_predictions,gamma,delta,False,\"Profit\",\"Profit-insensitive\")\n",
    "        \"\"\"\n",
    "        Average profit:\n",
    "        \"\"\"\n",
    "        profit_sensitive.append(prof_table_sensitive[fold].iloc[:,1].mean())\n",
    "        profit_insensitive.append(prof_table_insensitive[fold].iloc[:,1].mean())\n",
    "        \"\"\"\n",
    "        Max. profit and fractions\n",
    "        \"\"\"\n",
    "        list_sen_prof.append(prof_table_sensitive[fold].iloc[:,1])\n",
    "        list_ins_prof.append(prof_table_insensitive[fold].iloc[:,1])\n",
    "        \n",
    "    max_profit_sensitive = pd.concat(list_sen_prof, axis = 1)\n",
    "    max_profit_sensitive['mean'] = max_profit_sensitive.mean(axis=1)\n",
    "    MP_sen_index.append(max_profit_sensitive['mean'].idxmax())\n",
    "    MP_sen_val.append(max(max_profit_sensitive['mean']))\n",
    "\n",
    "    max_profit_insensitive = pd.concat(list_ins_prof, axis = 1)\n",
    "    max_profit_insensitive['mean'] = max_profit_insensitive.mean(axis=1)\n",
    "    MP_in_index.append(max_profit_insensitive['mean'].idxmax())\n",
    "    MP_in_val.append(max(max_profit_insensitive['mean']))\n",
    "        \n",
    "    # Qini results\n",
    "    av_qini_insensitive.append(statistics.mean(qini_insensitive))\n",
    "    av_qini_sensitive.append(statistics.mean(qini_sensitive))\n",
    "    std_qini_insensitive.append(statistics.stdev(qini_insensitive))\n",
    "    std_qini_sensitive.append(statistics.stdev(qini_sensitive))\n",
    "    \n",
    "    # Profit results\n",
    "    av_profit_insensitive.append(statistics.mean(profit_insensitive))\n",
    "    av_profit_sensitive.append(statistics.mean(profit_sensitive))\n",
    "    std_profit_insensitive.append(statistics.stdev(profit_insensitive))\n",
    "    std_profit_sensitive.append(statistics.stdev(profit_sensitive))\n",
    "    \n",
    "print(\"Qini_in: \",av_qini_insensitive) \n",
    "print(\"Qini_sen: \",av_qini_sensitive)\n",
    "#print(std_qini_insensitive)\n",
    "#print(std_qini_sensitive)\n",
    "print(\"Prof_in: \",av_profit_insensitive) \n",
    "print(\"Prof_se: \",av_profit_sensitive)\n",
    "#print(std_profit_insensitive)\n",
    "#print(std_profit_sensitive)\n",
    "print(\"MP_in_val: \",MP_in_val)\n",
    "print(\"MP_sen_val: \",MP_sen_val)\n",
    "print(\"MP_in_index: \",MP_in_index)\n",
    "print(\"MP_sen_index: \",MP_sen_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot profit\n",
    "\"\"\"\n",
    "performance_to_plot_sensitive = performance_profit(model_predictions,gamma,delta,True,\"profit\",\"Profit-sensitive\")\n",
    "performance_to_plot_insensitive = performance_profit(model_predictions,gamma,delta,False,\"profit\",\"Profit-insensitive\")\n",
    "performance_to_plot_sensitive = pd.melt(pd.concat(performance_to_plot_sensitive), ['x'])\n",
    "performance_to_plot_insensitive = pd.melt(pd.concat(performance_to_plot_insensitive),['x'])\n",
    "performance_to_plot = pd.concat([performance_to_plot_sensitive,performance_to_plot_insensitive])\n",
    "performance_to_plot.columns = ['x','Approach',\"Value\"]\n",
    "sns.set_style(\"white\")\n",
    "flatui = [\"#1f77b4\", \"#2ca02c\"]\n",
    "ax = sns.lineplot(x='x', y= 'Value', hue='Approach', palette=flatui,lw=1,data = performance_to_plot)\n",
    "plt.xlabel(\"Targeted proportion\")\n",
    "plt.ylabel(\"Profit per instance\")\n",
    "#plt.xticks(np.linspace(0, 100, 11, endpoint = True))\n",
    "#ax.legend(loc = 4, frameon = False, ncol = 1, labelspacing= 0.2)\n",
    "#plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "#plt.legend(loc=2, borderaxespad=0.1)#loc = 4\n",
    "#fig_uplift = ax.get_figure()\n",
    "#fig_uplift.savefig('b10_CF_Criteo.pdf',bbox_inches='tight',transparent =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Distribution P11\n",
    "\"\"\"\n",
    "import seaborn as sns\n",
    "\n",
    "effect_estimates= load(\"effect_estimates_LR.sav\")\n",
    "model_predictions = effect_estimates[1]\n",
    "\n",
    "perf_dist = performance_profit(model_predictions,gamma,delta,True,\"profit\",\"Profit-sensitive\")\n",
    "perf_dist = pd.concat(perf_dist, axis = 0)\n",
    "perf_dist_t = perf_dist.loc[(perf_dist[T] == 1) & (perf_dist[Y] == 1)]\n",
    "perf_dist_c = perf_dist.loc[(perf_dist[T] == 0) & (perf_dist[Y] == 1)]\n",
    "\n",
    "ax = sns.distplot( perf_dist_t[\"Prob_treat\"] , color=\"#1f77b4\")#,shade=True\n",
    "sns.distplot( perf_dist_c[\"Prob_treat\"] , color=\"#2ca02c\")#,shade=True\n",
    "plt.xlim(0, 1+0.05)\n",
    "plt.xlabel(\"P11\")\n",
    "fig_den = ax.get_figure()\n",
    "fig_den.legend(labels=['Treatment','Control'],bbox_to_anchor=(0.075, 0.87), loc=2, borderaxespad=0.)\n",
    "fig_den.savefig('Syn_den_P11.pdf',bbox_inches='tight',transparent =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Distribution ITE\n",
    "\"\"\"\n",
    "ax = sns.distplot( perf_dist_t[\"s\"] , color=\"#1f77b4\")#,shade=True\n",
    "sns.distplot( perf_dist_c[\"s\"] , color=\"#2ca02c\")#,shade=True\n",
    "plt.xlim(-1, 1)\n",
    "plt.xlabel(\"ITE\")\n",
    "fig_den = ax.get_figure()\n",
    "fig_den.legend(labels=['Treatment','Control'],bbox_to_anchor=(0.075, 0.87), loc=2, borderaxespad=0.)\n",
    "fig_den.savefig('Syn_den_ITE.pdf',bbox_inches='tight',transparent =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plots of cumulative positive instances in T and C\n",
    "\"\"\"\n",
    "performance_to_plot_sensitive = performance_profit(model_predictions,gamma,delta,True,\"qini\",\"Sensitive\")\n",
    "performance_to_plot_insensitive = performance_profit(model_predictions,gamma,delta,False,\"qini\",\"Insensitive\")\n",
    "performance_to_plot_sensitive = pd.melt(pd.concat(performance_to_plot_sensitive), ['x'])\n",
    "performance_to_plot_insensitive = pd.melt(pd.concat(performance_to_plot_insensitive),['x'])\n",
    "performance_to_plot = pd.concat([performance_to_plot_sensitive,performance_to_plot_insensitive])\n",
    "performance_to_plot.columns = ['x','Approach',\"Value\"]\n",
    "sns.set_style(\"white\")\n",
    "ax = sns.lineplot(x='x', y= 'Value', hue='Approach', style = 'Approach',lw=1,data = performance_to_plot)\n",
    "plt.xlabel(\"Targeted proportion\")\n",
    "plt.ylabel(\"Cumulative number of positive instances\")\n",
    "fig_uplift = ax.get_figure()\n",
    "fig_uplift.savefig('Syn_cum_trt_b10.pdf',bbox_inches='tight',transparent =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#effect_estimates= load(\"effect_estimates_LR.sav\")\n",
    "#model_predictions = effect_estimates[1]\n",
    "df_plot = performance_profit(model_predictions,gamma,delta,True,\"profit\",\"Profit-sensitive\")[0]\n",
    "sub_df_plot = df_plot.loc[(df_plot[\"ranking\"] >= 1) & (df_plot[\"ranking\"] <= 3000)]\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.scatter(sub_df_plot['Prob_treat'], sub_df_plot['s'], marker='o', c = sub_df_plot['ranking'],cmap='viridis');\n",
    "plt.plot(sub_df_plot['Prob_treat'], delta*sub_df_plot['Prob_treat'] + gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot_in = performance_profit(model_predictions,gamma,delta,False,\"profit\",\"Profit-sensitive\")[0]\n",
    "sub_df_plot = df_plot_in.loc[(df_plot[\"ranking\"] >= 1) & (df_plot[\"ranking\"] <= 1998)]\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.scatter(sub_df_plot['Prob_treat'], sub_df_plot['s'], marker='o', c = sub_df_plot['ranking'],cmap='viridis');\n",
    "plt.plot(sub_df_plot['Prob_treat'], delta*sub_df_plot['Prob_treat'] + gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot the Qini curves\n",
    "\"\"\"\n",
    "performance_to_plot_sensitive = performance_profit(model_predictions,gamma,delta,True,\"qini\",\"Profit-sensitive\")\n",
    "performance_to_plot_insensitive = performance_profit(model_predictions,gamma,delta,False,\"qini\",\"Profit-insensitive\")\n",
    "performance_to_plot_sensitive = pd.melt(pd.concat(performance_to_plot_sensitive), ['x'])\n",
    "performance_to_plot_insensitive = pd.melt(pd.concat(performance_to_plot_insensitive),['x'])\n",
    "performance_to_plot = pd.concat([performance_to_plot_sensitive,performance_to_plot_insensitive])\n",
    "performance_to_plot.columns = ['x','Approach',\"Value\"]\n",
    "sns.set_style(\"white\")\n",
    "ax = sns.lineplot(x='x', y= 'Value', hue='Approach', style = 'Approach',lw=1,data = performance_to_plot)\n",
    "plt.xlabel(\"Targeted proportion\")\n",
    "plt.ylabel(\"Cumulative Qini\")\n",
    "#plt.xticks(np.linspace(0, 100, 11, endpoint = True))\n",
    "#ax.legend(loc = 4, frameon = False, ncol = 1, labelspacing= 0.2)\n",
    "#plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.legend(loc=4, borderaxespad=0.1)\n",
    "#fig_uplift = ax.get_figure()\n",
    "#fig_uplift.savefig('Qini_Criteo_second.pdf',bbox_inches='tight',transparent =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Estimates the Qini and the profit\n",
    "\"\"\"\n",
    "def performance_profit(model_predictions,gamma,delta,frontier,metric,approach_name):\n",
    "    \n",
    "    \"\"\"\n",
    "    Lenght of the bins: here is 100 but can be any number\n",
    "    \"\"\"\n",
    "    len_fold = []\n",
    "    for fold in list(range(len(model_predictions))):\n",
    "        len_fold.append(len(model_predictions[fold]))\n",
    "    min_lenght_folds = min(len_fold)\n",
    "    min_lenght_folds = 100\n",
    "    \n",
    "    \"\"\"\n",
    "    Obtain the performance table:\n",
    "    \"\"\"\n",
    "    def performance_table(df,min_lenght_folds,frontier,metric,approach_name):\n",
    "        if frontier == True:\n",
    "            df['tau'] = gamma + delta*df['Prob_treat']\n",
    "            df['target'] = np.where(df['s'] >= df['tau'],1,-1)\n",
    "            #df['d'] = abs(delta*df.loc[:,'Prob_treat'] - df.loc[:,'s']+gamma)/(math.sqrt((delta**2)+1))#*df['target']\n",
    "            df['d'] = (df.loc[:,'s'] - delta*df.loc[:,'Prob_treat'] - gamma)/(math.sqrt((delta**2)+1))\n",
    "            df_sorted = df.sort_values(by = [\"d\"], ascending = False).reset_index(drop=True)\n",
    "        else:\n",
    "            df_sorted = df.sort_values(by = [\"s\"], ascending = False).reset_index(drop=True)\n",
    "        \n",
    "        df_sorted[\"ranking\"] =  list(range(1,len(df_sorted)+1))\n",
    "        labels = list(range(1,min_lenght_folds+1))\n",
    "        df_sorted[\"bin\"] = pd.cut(df_sorted['ranking'], len(labels), labels = labels).astype(int)\n",
    "        df_sorted.index = df_sorted.index + 1\n",
    "        df_sorted[\"ct\"] = np.where(df_sorted[T]==0,1,0)\n",
    "        df_sorted[\"y_tr\"] = np.where((df_sorted[T]==1) & (df_sorted[Y]==1) ,1,0)\n",
    "        df_sorted[\"y_ct\"] = np.where((df_sorted[T]==0) & (df_sorted[Y]==1) ,1,0)\n",
    "        n_t = df_sorted.pivot_table(index='bin', values= T, aggfunc='sum')\n",
    "        n_c = df_sorted.pivot_table(index='bin', values= \"ct\", aggfunc='sum')\n",
    "        n_y1_t = df_sorted.pivot_table(index='bin', values= 'y_tr', aggfunc='sum')\n",
    "        n_y1_c = df_sorted.pivot_table(index='bin', values= \"y_ct\", aggfunc='sum')\n",
    "        s_t = pd.concat([n_c,n_t,n_y1_c,n_y1_t], axis = 1)\n",
    "        s_t.columns = [\"n_c\",\"n_t\",\"n_y1_c\",\"n_y1_t\"]\n",
    "        s_t[\"n_y0_t\"] = s_t[\"n_t\"] - s_t[\"n_y1_t\"]\n",
    "        s_t[\"n_y0_c\"] = s_t[\"n_c\"] - s_t[\"n_y1_c\"]\n",
    "        s_t[\"bin\"] = s_t.index\n",
    "        s_t[\"cumsum_n_t\"] = s_t[\"n_t\"].cumsum()\n",
    "        s_t[\"cumsum_n_c\"] = s_t[\"n_c\"].cumsum()\n",
    "        s_t[\"cumsum_n_y1_t\"] = s_t[\"n_y1_t\"].cumsum()\n",
    "        s_t[\"cumsum_n_y1_c\"] = s_t[\"n_y1_c\"].cumsum()\n",
    "        s_t[\"cumsum_n_y0_t\"] = s_t[\"n_y0_t\"].cumsum()\n",
    "        s_t[\"cumsum_n_y0_c\"] = s_t[\"n_y0_c\"].cumsum()\n",
    "        \n",
    "        s_t[\"e11\"] = s_t[\"cumsum_n_y1_t\"]/s_t[\"cumsum_n_t\"].iloc[-1]\n",
    "        s_t[\"e10\"] = s_t[\"cumsum_n_y1_c\"]/s_t[\"cumsum_n_c\"].iloc[-1]\n",
    "        s_t[\"e01\"] = s_t[\"cumsum_n_y0_t\"]/s_t[\"cumsum_n_t\"].iloc[-1]\n",
    "        s_t[\"e00\"] = s_t[\"cumsum_n_y0_c\"]/s_t[\"cumsum_n_c\"].iloc[-1]\n",
    "        s_t[\"e11-e10\"] = s_t[\"e11\"] - s_t[\"e10\"]\n",
    "        \n",
    "        \"\"\"\n",
    "        Calculate the Qini at each threshold\n",
    "        \"\"\"\n",
    "        s_t[\"qini\"] = s_t[\"cumsum_n_y1_t\"] - (s_t[\"cumsum_n_y1_c\"]*s_t[\"cumsum_n_t\"])/s_t[\"cumsum_n_c\"]\n",
    "        s_t = s_t.replace(to_replace = np.nan, value = 0) \n",
    "        overall_inc_gains = s_t[\"qini\"].iloc[-1]\n",
    "        random_inc_gains = np.append([0],np.cumsum(np.repeat(overall_inc_gains / (s_t.loc[s_t.index[-1], \"bin\"]),(s_t.loc[s_t.index[-1], \"bin\"]))))\n",
    "        \n",
    "        \"\"\"\n",
    "        Calculate the profit at each threshold:\n",
    "        \"\"\"\n",
    "        net_ben_T_1 = OB[1][1] - TC[1][1]\n",
    "        net_ben_C_1 = OB[1][0] - TC[1][0]\n",
    "        net_ben_T_0 = OB[0][1] - TC[0][1]\n",
    "        net_ben_C_0 = OB[0][0] - TC[0][0]\n",
    "        s_t['profit'] = s_t[\"e11\"]*net_ben_T_1 - s_t[\"e10\"]*net_ben_C_1 + s_t[\"e01\"]*net_ben_T_0 - s_t[\"e00\"]*net_ben_C_0\n",
    "        #s_t['profit'] = s_t[\"e11-e10\"]*OB[1][1] - s_t[\"e11\"]*TC[1][1] - s_t[\"e01\"]*TC[0][1]\n",
    "        s_t = s_t.replace(to_replace = np.nan, value = 0) \n",
    "\n",
    "        # Qini\n",
    "        qini = s_t[\"qini\"]\n",
    "        qini = pd.Series(qini).values\n",
    "        qini = np.append([0],qini)\n",
    "\n",
    "        # Profit\n",
    "        profit = s_t['profit']\n",
    "        profit = pd.Series(profit).values\n",
    "        profit = np.append([0],profit)\n",
    "\n",
    "        # X axis\n",
    "        x = s_t[\"bin\"]/(s_t.loc[s_t.index[-1], \"bin\"])\n",
    "        x = pd.Series(x).values\n",
    "        x = np.append([0],x)\n",
    "        \n",
    "        if metric == \"qini\":\n",
    "            performance_table = pd.DataFrame(list(zip(x,qini, random_inc_gains)),columns =['x',approach_name, 'Random'])\n",
    "        else:\n",
    "            performance_table = pd.DataFrame(list(zip(x,profit)),columns =['x',approach_name])\n",
    "               \n",
    "        \"\"\"\n",
    "        Testing:\n",
    "        \"\"\"\n",
    "        cum_nt = s_t[\"cumsum_n_y1_t\"]\n",
    "        cum_nt = pd.Series(cum_nt).values\n",
    "        cum_nt = np.append([0],cum_nt)\n",
    "        perf_testing = pd.DataFrame(list(zip(x,cum_nt)),columns =['x',approach_name])\n",
    "        perf_testing[approach_name] = perf_testing[approach_name]/perf_testing[approach_name].iloc[-1]\n",
    "        \n",
    "        \"\"\"\n",
    "        For density plots of P11 and ITE\n",
    "        \"\"\"\n",
    "        p11_ITEs = pd.concat([df[Y],df[T],df['Prob_treat'],df['s']], axis = 1)\n",
    "        \n",
    "        return p11_ITEs;\n",
    "    \n",
    "    performance_tables_fold = []\n",
    "    for fold in list(range(len(model_predictions))):\n",
    "        performance_tables_fold.append(performance_table(model_predictions[fold],min_lenght_folds,frontier,metric,approach_name))\n",
    "    return performance_tables_fold;\n",
    "\n",
    "\"\"\"\n",
    "Calculate the qini metric (scalar value)\n",
    "\"\"\"\n",
    "\n",
    "def qini_metric(perf_object):\n",
    "    x = list(perf_object.iloc[:,0]/100)\n",
    "    y_inc = list(perf_object.iloc[:,1])\n",
    "    y_ran = list(perf_object.iloc[:,2])\n",
    "    \n",
    "    def auc(x,y):\n",
    "        auc = 0\n",
    "        for i in list(range(1,len(x))):\n",
    "            auc = auc + 0.5 * (x[i] - x[i-1]) * (y[i] + y[i-1])\n",
    "        return auc\n",
    " \n",
    "    auc_inc = auc(x,y_inc)\n",
    "    auc_ran = auc(x,y_ran)\n",
    "    \n",
    "    qini = auc_inc - auc_ran\n",
    "    \n",
    "    return qini;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
